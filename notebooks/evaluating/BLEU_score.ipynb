{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Bilingual Evaluation Understudy (BLEU) score___\n",
    "\n",
    "### __Deep Learning__\n",
    "\n",
    "#### __Project: Image Captioning with Visual Attention__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ[\"PYTHONPATH\"])\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "import scripts.data_loading as dl\n",
    "import scripts.data_processing as dp\n",
    "from scripts import model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.09s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "root = dl.DATASET_PATHS[dl.DatasetType.VALIDATION].images\n",
    "ann_json = dl.DATASET_PATHS[dl.DatasetType.VALIDATION].captions_json\n",
    "\n",
    "vocabulary = dp.Vocabulary()\n",
    "\n",
    "coco_val = torchvision.datasets.CocoCaptions(root, ann_json, dp.VGGNET_PREPROCESSING_PIPELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, captions = coco_val[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A crowd of people standing around each other.',\n",
       " 'Someone is holding a large teddy bear in the crowd.',\n",
       " 'A group of men and women gathered together outside. ',\n",
       " 'A large crowd of people walking around while one owns a teddy bear.',\n",
       " 'A large group of people on a city street.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'crowd', 'of', 'people', 'standing', 'around', 'each', 'other']\n",
      "['someone', 'is', 'holding', 'a', 'large', 'teddy', 'bear', 'in', 'the', 'crowd']\n",
      "['a', 'group', 'of', 'men', 'and', 'women', 'gathered', 'together', 'outside']\n",
      "['a', 'large', 'crowd', 'of', 'people', 'walking', 'around', 'while', 'one', 'owns', 'a', 'teddy', 'bear']\n",
      "['a', 'large', 'group', 'of', 'people', 'on', 'a', 'city', 'street']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_captions = []\n",
    "for caption in captions:\n",
    "    preprocessed_caption = dp.TextPipeline.normalize(caption).split()\n",
    "    preprocessed_captions.append(preprocessed_caption)\n",
    "    print(preprocessed_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.VGG19Encoder()\n",
    "decoder = model.LSTMDecoder(\n",
    "    num_embeddings=len(vocabulary),\n",
    "    embedding_dim=8,\n",
    "    encoder_dim=196,\n",
    "    decoder_dim=16,\n",
    "    attention_dim=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps, feature_mean = encoder.forward(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence, _ = decoder.greedy_decoding(\n",
    "    feature_maps=feature_maps,\n",
    "    feature_mean=feature_mean,\n",
    "    start_token_index=vocabulary.word2idx(\"<SOS>\"),\n",
    "    end_token_index=vocabulary.word2idx(\"<EOS>\"),\n",
    "    max_length=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7415, 4186, 6536, 1821, 9678, 3774, 9767, 5687, 2204, 9904]\n"
     ]
    }
   ],
   "source": [
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toasters swimmers mango bin nuclear docking shoeless chaise mushroom suggesting'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = dp.TextPipeline.decode_caption(vocabulary, sequence)\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = ['people', 'people', 'people', 'people', 'people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraction(1, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.modified_precision(preprocessed_captions, sequence, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5488116360940264"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.brevity_penalty(8, len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiktorlazarski/Desktop/Image-Captioning-with-Visual-Attention/venv/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/wiktorlazarski/Desktop/Image-Captioning-with-Visual-Attention/venv/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/wiktorlazarski/Desktop/Image-Captioning-with-Visual-Attention/venv/lib/python3.8/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10976232721880529"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.sentence_bleu(references=preprocessed_captions, hypothesis=sequence, weights=(1.0, 0.0, 0.0, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'the', 'the', 'the', 'the', 'the', 'the']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp = [\"the\"] * 7\n",
    "hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'cat', 'is', 'on', 'the', 'mat'],\n",
       " ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]]\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2857142857142857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.sentence_bleu(references=ref, hypothesis=hyp, weights=(1, 0, 0, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
