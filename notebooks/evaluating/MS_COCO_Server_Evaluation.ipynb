{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __MS COCO Server Evaluation__\n",
    "\n",
    "### __Deep Learning__\n",
    "\n",
    "#### __Project: Image Captioning with Visual Attention__\n",
    "\n",
    "This notebook generates json files ready to be uploaded to MS COCO Evaluation server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.environ[\"PYTHONPATH\"])\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "import scripts.data_loading as dl\n",
    "import scripts.data_processing as dp\n",
    "from scripts import model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./models/checkpoints/decoder_lr_3e-04_dropout_0.5_lambda_0.0.pth\"\n",
    "\n",
    "RESULT_DIRECTORY = \"./results\"\n",
    "VALIDATION_DATASET_CAPTIONS_JSON = os.path.join(RESULT_DIRECTORY, \"val2014_cap.json\")\n",
    "TEST_DATASET_CAPTIONS_JSON = os.path.join(RESULT_DIRECTORY, \"test2014_cap.json\")\n",
    "\n",
    "COCO_VAL14_DATASET_IMAGES = \"./data/validation/val2014\"\n",
    "COCO_VAL14_DATASET_ANNOTATIONS = \"./data/validation/captions_val2014.json\"\n",
    "\n",
    "COCO_TEST14_DATASET_IMAGES = \"./data/test/test2014\"\n",
    "COCO_TEST14_DATASET_ANNOTATIONS = \"./data/test/image_info_test2014.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_decoder(state_path: str) -> model.LSTMDecoder:\n",
    "    decoder_state = torch.load(state_path)\n",
    "    \n",
    "    decoder = model.LSTMDecoder(num_embeddings=10_004, embedding_dim=128, encoder_dim=196, decoder_dim=512, attention_dim=256)\n",
    "    decoder.load_state_dict(decoder_state[\"decoder\"])\n",
    "    decoder.to(device)\n",
    "\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.VGG19Encoder()\n",
    "encoder.to(device)\n",
    "decoder = load_decoder(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = dp.Vocabulary()\n",
    "beam_size = 3\n",
    "num_sequences = 5\n",
    "\n",
    "images_directory = COCO_VAL14_DATASET_IMAGES\n",
    "ann_file = COCO_VAL14_DATASET_ANNOTATIONS\n",
    "out_file = VALIDATION_DATASET_CAPTIONS_JSON\n",
    "\n",
    "coco = COCO(ann_file)\n",
    "\n",
    "# create json file\n",
    "evaluation_results = []\n",
    "for i, image_id in enumerate(coco.imgs):\n",
    "    img_filename = coco.loadImgs(image_id)[0][\"file_name\"]\n",
    "    img_path = os.path.join(images_directory, img_filename)\n",
    "    \n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = dp.VGGNET_PREPROCESSING_PIPELINE(image)\n",
    "    \n",
    "    image = image.to(device)\n",
    "    \n",
    "    feature_maps, feature_mean = encoder(image.unsqueeze(0))\n",
    "    captions = decoder.beam_search(\n",
    "        feature_maps=feature_maps,\n",
    "        feature_mean=feature_mean,\n",
    "        start_token_index=vocabulary.word2idx(\"<SOS>\"),\n",
    "        end_token_index=vocabulary.word2idx(\"<EOS>\"),\n",
    "        beam_size=beam_size,\n",
    "        num_sequences=num_sequences,\n",
    "        max_length=100\n",
    "    )\n",
    "    \n",
    "    decoded_captions = []\n",
    "    for sequence, score in captions:\n",
    "        sequence = dp.TextPipeline.decode_caption(vocabulary, sequence)\n",
    "        sequence = (sequence, score)\n",
    "        \n",
    "        decoded_captions.append(sequence)\n",
    "    \n",
    "    best_caption = sorted(decoded_captions, reverse=True, key=lambda x: x[1])[0][0]\n",
    "    \n",
    "    result_dict = {\n",
    "        \"image_id\": image_id,\n",
    "        \"caption\": best_caption\n",
    "    }\n",
    "    evaluation_results.append(result_dict)\n",
    "\n",
    "    \n",
    "os.makedirs(RESULT_DIRECTORY, exist_ok=True)\n",
    "with open(out_file, 'w') as out_json:\n",
    "    json.dump(evaluation_results, out_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_val_results = coco.loadRes(VALIDATION_DATASET_CAPTIONS_JSON)\n",
    "\n",
    "cocoEval = COCOEvalCap(coco, coco_val_results)\n",
    "cocoEval.params['image_id'] = coco_val_results.getImgIds()\n",
    "\n",
    "cocoEval.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
